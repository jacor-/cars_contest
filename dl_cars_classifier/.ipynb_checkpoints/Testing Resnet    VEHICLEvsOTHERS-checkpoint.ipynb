{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = (15.0, 30.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prepare paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### General data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset import get_positive_patches, get_negative_patches, get_labels, load_image\n",
    "from common import dataset, utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "labels_data, data = dataset.labels_data, dataset.data\n",
    "\n",
    "\n",
    "## This one contains separatedly cars, vans...\n",
    "#vehicle_types = set(dataset.map_category.values())\n",
    "#vehicle_types.add('Empty')\n",
    "\n",
    "map_category = {'A':'Vehicle', 'B':'Vehicle', 'C':'Vehicle', 'D':'Vehicle', 'E':'Vehicle', 'F':'Vehicle', 'G':'Vehicle', 'H':'Vehicle', 'I':'Vehicle'}\n",
    "labels_data = pd.read_csv('../data/trainingObservations.csv')\n",
    "data = get_labels(labels_data, map_category)\n",
    "vehicle_types = set(map_category.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_images = 60\n",
    "#cases = list(data.keys())\n",
    "#np.random.shuffle(cases)\n",
    "#train_cases, test_cases = cases[test_images:], cases[:test_images]\n",
    "#np.save('../data/patch_images/train_cases.npy', train_cases)\n",
    "#np.save('../data/patch_images/test_cases.npy', test_cases)\n",
    "train_cases = np.load('../data/patch_images/train_cases.npy')\n",
    "test_cases  = np.load('../data/patch_images/test_cases.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset and the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save positive and negative images in different folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import os\n",
    "'''\n",
    "for traintest_case in (['train','test']):\n",
    "    number_of_negative_per_image = 50\n",
    "\n",
    "    ## Delete all the folders\n",
    "    for key in list(vehicle_types):\n",
    "        os.system('rm -rf ../data/patch_images/%s/%s || True' % (traintest_case, key))\n",
    "\n",
    "    # Create new folders\n",
    "    for key in list(vehicle_types):\n",
    "        os.system('mkdir ../data/patch_images/%s/%s || True' % (traintest_case, key))\n",
    "\n",
    "    # Generate positive samples and save them in the proper folder\n",
    "    if traintest_case == 'train':\n",
    "        files = train_cases\n",
    "    elif traintest_case == 'test':\n",
    "        files = test_cases\n",
    "    else:\n",
    "        raise 'Inutil! Train o test!'\n",
    "        \n",
    "    for case in files:\n",
    "        for key in list(vehicle_types):\n",
    "            pos = get_positive_patches(dataset.load_image(case), data[case], 110, [key])    \n",
    "            for j in range(pos.shape[0]):\n",
    "                scipy.misc.imsave('../data/patch_images/%s/%s/%s_%d.jpg' % (traintest_case,key,case,j), pos[j])\n",
    "\n",
    "    # Generate negative samples and save them in the proper folder\n",
    "    os.system('rm -rf ../data/patch_images/%s/Empty' % traintest_case)\n",
    "    os.system('mkdir ../data/patch_images/%s/Empty' % traintest_case)\n",
    "    for case in files:\n",
    "        pos = get_negative_patches(dataset.load_image(case), data[case], 110, number_of_negative_per_image)\n",
    "        for j in range(pos.shape[0]):\n",
    "            scipy.misc.imsave('../data/patch_images/%s/%s/%s_%d.jpg' % (traintest_case,'Empty',case,j), pos[j])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls /fjord/jose/cars_data/patch_images/train/Empty/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls /fjord/jose/cars_data/patch_images/train/Vehicle/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls /fjord/jose/cars_data/patch_images/test/Empty/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606\r\n"
     ]
    }
   ],
   "source": [
    "ls /fjord/jose/cars_data/patch_images/test/Vehicle/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vehicle_types.add('Empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator_ready\n"
     ]
    }
   ],
   "source": [
    "image_size_nn = 48\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "def generate_training_cases(batch_size, case):\n",
    "    if case not in ('train','test'):\n",
    "        raise \"Only train o test\"\n",
    "    datagen = ImageDataGenerator(rescale=1./255, vertical_flip=True, horizontal_flip = True, zoom_range = 0.02, rotation_range=180)\n",
    "    a = 0\n",
    "    for X_batch, y_batch in datagen.flow_from_directory('../data/patch_images/%s'%case, target_size = [image_size_nn,image_size_nn], class_mode = 'binary', classes = vehicle_types, batch_size=batch_size):\n",
    "        yield X_batch, y_batch\n",
    "        \n",
    "#a = 0\n",
    "#for X_batch, y_batch in generate_training_cases(32):\n",
    "#    #utils.multiplot(X_batch.transpose([0,2,3,1]), [10,5])\n",
    "#    print(y_batch.sum(axis = 0))\n",
    "#    a+=1\n",
    "#    if a == 10:\n",
    "#        break\n",
    "\n",
    "train_generator = generate_training_cases(32, 'train')\n",
    "test_generator = generate_training_cases(32, 'test')\n",
    "print('generator_ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33208 images belonging to 2 classes.\n",
      "27 cars out of 32 samples in train generator\n",
      "24 cars out of 32 samples in train generator\n",
      "27 cars out of 32 samples in train generator\n",
      "27 cars out of 32 samples in train generator\n",
      "22 cars out of 32 samples in train generator\n",
      "28 cars out of 32 samples in train generator\n",
      "28 cars out of 32 samples in train generator\n",
      "27 cars out of 32 samples in train generator\n",
      "25 cars out of 32 samples in train generator\n",
      "29 cars out of 32 samples in train generator\n",
      "22 cars out of 32 samples in train generator\n",
      "28 cars out of 32 samples in train generator\n",
      "26 cars out of 32 samples in train generator\n",
      "28 cars out of 32 samples in train generator\n",
      "28 cars out of 32 samples in train generator\n",
      "22 cars out of 32 samples in train generator\n",
      "27 cars out of 32 samples in train generator\n",
      "25 cars out of 32 samples in train generator\n",
      "25 cars out of 32 samples in train generator\n",
      "28 cars out of 32 samples in train generator\n",
      " --- \n",
      "Found 3606 images belonging to 2 classes.\n",
      "26 cars out of 32 samples in test generator\n",
      "25 cars out of 32 samples in test generator\n",
      "27 cars out of 32 samples in test generator\n",
      "28 cars out of 32 samples in test generator\n",
      "26 cars out of 32 samples in test generator\n",
      "21 cars out of 32 samples in test generator\n",
      "29 cars out of 32 samples in test generator\n",
      "28 cars out of 32 samples in test generator\n",
      "26 cars out of 32 samples in test generator\n",
      "25 cars out of 32 samples in test generator\n",
      "27 cars out of 32 samples in test generator\n",
      "28 cars out of 32 samples in test generator\n",
      "29 cars out of 32 samples in test generator\n",
      "28 cars out of 32 samples in test generator\n",
      "25 cars out of 32 samples in test generator\n",
      "25 cars out of 32 samples in test generator\n",
      "28 cars out of 32 samples in test generator\n",
      "28 cars out of 32 samples in test generator\n",
      "28 cars out of 32 samples in test generator\n",
      "28 cars out of 32 samples in test generator\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for _,y in train_generator:\n",
    "    print(\"%d cars out of %d samples in train generator\" % (np.sum(y), y.shape[0]))\n",
    "    counter += 1\n",
    "    if counter == 20:\n",
    "        break\n",
    "\n",
    "print(\" --- \")\n",
    "\n",
    "counter = 0        \n",
    "for _,y in test_generator:\n",
    "    print(\"%d cars out of %d samples in test generator\" % (np.sum(y), y.shape[0]))\n",
    "    counter += 1\n",
    "    if counter == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "### Train a resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../dl_networks/resnet.py:44: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(kernel_size=(7, 7), filters=64, strides=(2, 2), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  init=init, border_mode=border_mode, W_regularizer=W_regularizer)(input)\n",
      "../dl_networks/resnet.py:211: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")`\n",
      "  pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), border_mode=\"same\")(conv1)\n",
      "../dl_networks/resnet.py:121: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(kernel_size=(3, 3), filters=64, strides=(1, 1), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  W_regularizer=l2(0.0001))(input)\n",
      "../dl_networks/resnet.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(kernel_size=(3, 3), filters=64, strides=(1, 1), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  init=init, border_mode=border_mode, W_regularizer=W_regularizer)(activation)\n",
      "../dl_networks/resnet.py:91: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return merge([shortcut, residual], mode=\"sum\")\n",
      "/home/joseal/.local/lib/python3.6/site-packages/keras/legacy/layers.py:456: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "../dl_networks/resnet.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(kernel_size=(3, 3), filters=128, strides=(2, 2), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  init=init, border_mode=border_mode, W_regularizer=W_regularizer)(activation)\n",
      "../dl_networks/resnet.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(kernel_size=(3, 3), filters=128, strides=(1, 1), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  init=init, border_mode=border_mode, W_regularizer=W_regularizer)(activation)\n",
      "../dl_networks/resnet.py:89: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(kernel_size=(1, 1), filters=128, strides=(2, 2), padding=\"valid\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  W_regularizer=l2(0.0001))(input)\n",
      "../dl_networks/resnet.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(kernel_size=(3, 3), filters=256, strides=(2, 2), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  init=init, border_mode=border_mode, W_regularizer=W_regularizer)(activation)\n",
      "../dl_networks/resnet.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(kernel_size=(3, 3), filters=256, strides=(1, 1), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  init=init, border_mode=border_mode, W_regularizer=W_regularizer)(activation)\n",
      "../dl_networks/resnet.py:89: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(kernel_size=(1, 1), filters=256, strides=(2, 2), padding=\"valid\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  W_regularizer=l2(0.0001))(input)\n",
      "../dl_networks/resnet.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(kernel_size=(3, 3), filters=512, strides=(2, 2), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  init=init, border_mode=border_mode, W_regularizer=W_regularizer)(activation)\n",
      "../dl_networks/resnet.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(kernel_size=(3, 3), filters=512, strides=(1, 1), padding=\"same\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  init=init, border_mode=border_mode, W_regularizer=W_regularizer)(activation)\n",
      "../dl_networks/resnet.py:89: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(kernel_size=(1, 1), filters=512, strides=(2, 2), padding=\"valid\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  W_regularizer=l2(0.0001))(input)\n",
      "../dl_networks/resnet.py:222: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=1)`\n",
      "  block_norm = BatchNormalization(mode=0, axis=CHANNEL_AXIS)(block)\n",
      "../dl_networks/resnet.py:233: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"he_normal\")`\n",
      "  dense = Dense(output_dim=1, init=\"he_normal\", activation=\"sigmoid\")(flatten1)\n",
      "../dl_networks/resnet.py:234: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  model = Model(input=input, output=dense)\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:51: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., steps_per_epoch=10, verbose=1, callbacks=[<keras.ca..., validation_data=<generator..., validation_steps=10, max_q_size=128, epochs=500, workers=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "10/10 [==============================] - 16s - loss: 1.5031 - acc: 0.4906 - val_loss: 2.3922 - val_acc: 0.1844\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - 12s - loss: 1.2566 - acc: 0.7250 - val_loss: 2.0201 - val_acc: 0.1531\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - 12s - loss: 1.1593 - acc: 0.8500 - val_loss: 1.3697 - val_acc: 0.7125\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - 12s - loss: 1.0839 - acc: 0.8688 - val_loss: 1.3617 - val_acc: 0.7406\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - 12s - loss: 1.0001 - acc: 0.9313 - val_loss: 1.3158 - val_acc: 0.8406\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - 11s - loss: 0.9990 - acc: 0.9250 - val_loss: 1.3108 - val_acc: 0.8406\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - 11s - loss: 1.0075 - acc: 0.9062 - val_loss: 1.2518 - val_acc: 0.8938\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - 12s - loss: 0.9748 - acc: 0.9062 - val_loss: 1.2325 - val_acc: 0.8750\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - 11s - loss: 0.9755 - acc: 0.9094 - val_loss: 1.2085 - val_acc: 0.8742\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - 12s - loss: 0.9011 - acc: 0.9531 - val_loss: 1.2035 - val_acc: 0.8938\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - 11s - loss: 0.9261 - acc: 0.9437 - val_loss: 1.1841 - val_acc: 0.8688\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - 12s - loss: 0.8736 - acc: 0.9469 - val_loss: 1.1221 - val_acc: 0.9125\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - 12s - loss: 0.8556 - acc: 0.9531 - val_loss: 1.1452 - val_acc: 0.9000\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - 11s - loss: 0.8962 - acc: 0.9313 - val_loss: 1.1382 - val_acc: 0.9031\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - 11s - loss: 0.8939 - acc: 0.9344 - val_loss: 1.0799 - val_acc: 0.9219\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - 11s - loss: 0.8635 - acc: 0.9344 - val_loss: 1.0284 - val_acc: 0.9125\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - 11s - loss: 0.8980 - acc: 0.9406 - val_loss: 1.0284 - val_acc: 0.9094\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - 11s - loss: 0.9286 - acc: 0.9250 - val_loss: 1.0178 - val_acc: 0.8750\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - 11s - loss: 0.8723 - acc: 0.9437 - val_loss: 1.0008 - val_acc: 0.9031\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - 11s - loss: 0.8424 - acc: 0.9469 - val_loss: 1.0331 - val_acc: 0.9000\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - 11s - loss: 0.9056 - acc: 0.9406 - val_loss: 1.0050 - val_acc: 0.9031\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - 11s - loss: 0.8515 - acc: 0.9625 - val_loss: 1.0173 - val_acc: 0.8875\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - 11s - loss: 0.8950 - acc: 0.9250 - val_loss: 0.9619 - val_acc: 0.8969\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - 11s - loss: 0.8590 - acc: 0.9437 - val_loss: 0.9100 - val_acc: 0.9375\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - 12s - loss: 0.8719 - acc: 0.9375 - val_loss: 0.9311 - val_acc: 0.9094\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - 11s - loss: 0.9077 - acc: 0.9156 - val_loss: 0.8989 - val_acc: 0.9344\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - 12s - loss: 0.8072 - acc: 0.9625 - val_loss: 0.9299 - val_acc: 0.9531\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - 11s - loss: 0.8730 - acc: 0.9344 - val_loss: 0.9015 - val_acc: 0.9406\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - 11s - loss: 0.8623 - acc: 0.9406 - val_loss: 0.8528 - val_acc: 0.9531\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - 11s - loss: 0.8642 - acc: 0.9375 - val_loss: 0.8551 - val_acc: 0.9500\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - 11s - loss: 0.8288 - acc: 0.9531 - val_loss: 0.8640 - val_acc: 0.9469\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - 11s - loss: 0.8291 - acc: 0.9500 - val_loss: 0.8313 - val_acc: 0.9719\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - 11s - loss: 0.8142 - acc: 0.9563 - val_loss: 0.9018 - val_acc: 0.9250\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - 11s - loss: 0.8498 - acc: 0.9437 - val_loss: 0.8482 - val_acc: 0.9406\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - 11s - loss: 0.8178 - acc: 0.9719 - val_loss: 0.8441 - val_acc: 0.9437\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - 11s - loss: 0.8206 - acc: 0.9469 - val_loss: 0.7952 - val_acc: 0.9710\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - 11s - loss: 0.9114 - acc: 0.9156 - val_loss: 0.8318 - val_acc: 0.9531\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - 12s - loss: 0.8056 - acc: 0.9625 - val_loss: 0.8119 - val_acc: 0.9437\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - 12s - loss: 0.8029 - acc: 0.9563 - val_loss: 0.8206 - val_acc: 0.9594\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - 11s - loss: 0.8189 - acc: 0.9531 - val_loss: 0.8083 - val_acc: 0.9594\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - 12s - loss: 0.7827 - acc: 0.9531 - val_loss: 0.8034 - val_acc: 0.9625\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - 11s - loss: 0.7881 - acc: 0.9750 - val_loss: 0.7790 - val_acc: 0.9625\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - 11s - loss: 0.8232 - acc: 0.9375 - val_loss: 0.7645 - val_acc: 0.9656\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - 12s - loss: 0.7747 - acc: 0.9625 - val_loss: 0.7889 - val_acc: 0.9594\n",
      "Epoch 45/500\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7721 - acc: 0.9688"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"<ipython-input-13-f6e457619593>\"\u001b[0m, line \u001b[1;32m51\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    nb_worker=1)  # a locker is needed if increased the number of parallel workers\n",
      "  File \u001b[1;32m\"/home/joseal/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\"\u001b[0m, line \u001b[1;32m88\u001b[0m, in \u001b[1;35mwrapper\u001b[0m\n    return func(*args, **kwargs)\n",
      "  File \u001b[1;32m\"/home/joseal/.local/lib/python3.6/site-packages/keras/engine/training.py\"\u001b[0m, line \u001b[1;32m1876\u001b[0m, in \u001b[1;35mfit_generator\u001b[0m\n    class_weight=class_weight)\n",
      "  File \u001b[1;32m\"/home/joseal/.local/lib/python3.6/site-packages/keras/engine/training.py\"\u001b[0m, line \u001b[1;32m1620\u001b[0m, in \u001b[1;35mtrain_on_batch\u001b[0m\n    outputs = self.train_function(ins)\n",
      "  File \u001b[1;32m\"/home/joseal/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\"\u001b[0m, line \u001b[1;32m2073\u001b[0m, in \u001b[1;35m__call__\u001b[0m\n    feed_dict=feed_dict)\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\"\u001b[0m, line \u001b[1;32m767\u001b[0m, in \u001b[1;35mrun\u001b[0m\n    run_metadata_ptr)\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\"\u001b[0m, line \u001b[1;32m965\u001b[0m, in \u001b[1;35m_run\u001b[0m\n    feed_dict_string, options, run_metadata)\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\"\u001b[0m, line \u001b[1;32m1015\u001b[0m, in \u001b[1;35m_do_run\u001b[0m\n    target_list, options, run_metadata)\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\"\u001b[0m, line \u001b[1;32m1022\u001b[0m, in \u001b[1;35m_do_call\u001b[0m\n    return fn(*args)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\"\u001b[0;36m, line \u001b[0;32m1004\u001b[0;36m, in \u001b[0;35m_run_fn\u001b[0;36m\u001b[0m\n\u001b[0;31m    status, run_metadata)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import logging\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from dl_networks.resnet import ResnetBuilder\n",
    "from dl_utils.tb_callback import TensorBoard\n",
    "\n",
    "\n",
    "OUTPUT_MODEL = '../data/models/vehicle_empty_discriminator.hdf5'  # OUTPUT_MODEL = wp + 'personal/jm_patches_train_v06_local.hdf5'\n",
    "LOGS_PATH    = '../data/logs/%s' % str('vehicle_empty_resnet_v0')\n",
    "\n",
    "#LOGS_PATH = wp + 'logs/%s' % str(int(time()))\n",
    "if not os.path.exists(LOGS_PATH):\n",
    "    os.makedirs(LOGS_PATH)\n",
    "    \n",
    "K.set_image_dim_ordering('th')\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s  %(levelname)-8s %(message)s',\n",
    "                    datefmt='%m-%d %H:%M:%S')\n",
    "\n",
    "tb = TensorBoard(log_dir=LOGS_PATH, histogram_freq=1, write_graph=False, write_images=False)  # replace keras.callbacks.TensorBoard\n",
    "model_checkpoint = ModelCheckpoint(OUTPUT_MODEL, monitor='loss', save_best_only=True)\n",
    "\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = ResnetBuilder().build_resnet_50((3,image_size_nn,image_size_nn),1)\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', metrics=['accuracy'])#,'fmeasure'])\n",
    "# model.load_weights(OUTPUT_MODEL)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=50,  # make it small to update TB and CHECKPOINT frequently\n",
    "                    nb_epoch=500,\n",
    "                    verbose=1,\n",
    "                    #class_weight={0:1., 1:4.},\n",
    "                    callbacks=[model_checkpoint], #[tb, model_checkpoint],\n",
    "                    validation_data=test_generator,  # TODO: is_training=False\n",
    "                    validation_steps=10,\n",
    "                    max_q_size=128,\n",
    "                    nb_worker=1)  # a locker is needed if increased the number of parallel workers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "flow() got an unexpected keyword argument 'target_size'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-f20baec966c5>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    for X_batch, _ in datagen_eval.flow(patches, np.zeros(patches.shape[0]), shuffle = False, target_size = [image_size_nn,image_size_nn], class_mode = 'binary', classes = vehicle_types, batch_size=batch_size):\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m\u001b[0;31m:\u001b[0m flow() got an unexpected keyword argument 'target_size'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.image import extract_patches_2d, reconstruct_from_patches_2d\n",
    "import time\n",
    "\n",
    "\n",
    "datagen_eval = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "patch_size = 110\n",
    "pred_per_row = []\n",
    "for i_row in range(2000-patch_size):\n",
    "    print(i_row)\n",
    "    patches = extract_patches_2d(load_image(train_cases[0])[i_row:i_row+110,:], [patch_size,patch_size]).transpose([0,3,1,2])\n",
    "    for X_batch, _ in datagen_eval.flow(patches, np.zeros(patches.shape[0]), shuffle = False, target_size = [image_size_nn,image_size_nn], class_mode = 'binary', classes = vehicle_types, batch_size=batch_size):\n",
    "        out = model.predict(patches, batch_size = batch_size).flatten()\n",
    "        pred_per_row.append(out)\n",
    "\n",
    "print(time.time() - t1)\n",
    "#out = out.reshape([out.shape[0],1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reconstructed = reconstruct_from_patches_2d(out, [120-patch_size+1,120-patch_size+1])\n",
    "#imshow(reconstructed, cmap = cm.Greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
